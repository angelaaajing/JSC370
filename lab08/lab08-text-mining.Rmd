---
title: "Lab 08 - Text Mining/NLP"
author: "Jing Yu"
date: "2024-02-28"
output: 
  html_document: 
    fig_width: 10
    fig_height: 6
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(eval = T, include  = T)
```

# Learning goals

-   Use `unnest_tokens()` and `unnest_ngrams()` to extract tokens and ngrams from text
-   Use dplyr and ggplot2 to analyze and visualize text data
-   Try a theme model using `topicmodels`

# Lab description

For this lab we will be working with the medical record transcriptions from <https://www.mtsamples.com/>. And is loaded and "fairly" cleaned at <https://github.com/JSC370/JSC370-2024/tree/main/data/medical_transcriptions>.

This markdown document should be rendered using `github_document` document.

### Setup packages

You should load in `dplyr`, (or `data.table` if you want to work that way), `ggplot2` and `tidytext`. Install `wordcloud`, `tm`, and `topicmodels` if you don't alreadyh have them.

### Read in the Medical Transcriptions

Loading in reference transcription samples from <https://www.mtsamples.com/>

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(tidytext)
library(tidyverse)
library(wordcloud)
library(tm)
library(topicmodels)

mt_samples <- read_csv("https://raw.githubusercontent.com/JSC370/JSC370-2024/main/data/medical_transcriptions/mtsamples.csv")
mt_samples <- mt_samples %>%
  select(description, medical_specialty, transcription)

head(mt_samples)
```

------------------------------------------------------------------------

## Question 1: What specialties do we have?

We can use `count()` from `dplyr` to figure out how many different medical specialties are in the data. Are these categories related? overlapping? evenly distributed? Make a bar plot.

```{r}
mt_samples |>
  count(medical_specialty, sort = TRUE) |>
  ggplot(aes(fct_reorder(medical_specialty, n), n)) +
  labs(x = "Medical Specialty", y = "Frequency", title = "Distribution of Medical Specialties") +
  geom_col() +
  coord_flip()
```

The medical specialties does not seem to overlap with each other, and they are not evenly distributed. Surgery has a dominant presence in this dataset.

------------------------------------------------------------------------

## Question 2: Tokenize

-   Tokenize the the words in the `transcription` column
-   Count the number of times each token appears
-   Visualize the top 20 most frequent words with a bar plot
-   Create a word cloud

Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

```{r}
tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  group_by(word) |>
  summarise(word_frequency = n()) |>
  arrange(across(word_frequency, desc)) |>
  head(20)

tokens |>
  ggplot(aes(reorder(word, word_frequency), word_frequency)) +
  geom_bar(stat = "identity") +
  labs(x = "Word", y = "Frequency", title = "Top 20 Most Frequent Words") +
  coord_flip()

wordcloud(tokens$word, tokens$word_frequency)
```

We see that the words have high frequency in the data are the words such as "the" and "was", which has little information. We may need to remove these stopwords first.

------------------------------------------------------------------------

## Question 3: Stopwords

-   Redo Question 2 but remove stopwords(use stopwords package)
-   Bonus points if you remove numbers as well (use regex)

What do we see know that we have removed stop words? Does it give us a better idea of what the text is about?

-   Try customizing your stopwords list to include 3-4 additional words that do not appear informative

```{r}
head(stopwords("english"))
length(stopwords("english"))

tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  group_by(word) |>
  summarise(word_frequency = n()) |>
  filter(!(word %in% stopwords("english"))) |>
  filter(!(grepl("[[:digit:]]+", word))) |>
  arrange(across(word_frequency, desc)) |>
  head(20)

tokens |>
  ggplot(aes(reorder(word, word_frequency), word_frequency)) +
  geom_bar(stat = "identity") +
  labs(x = "Word", y = "Frequency", title = "Top 20 Most Frequent Words") +
  coord_flip()

wordcloud(tokens$word, tokens$word_frequency)
```

After removing the stopwords, we have a better sense of the content of the documents as we see word like “patient” and “anesthesia” appear in the bar plot.

```{r}
stopwords2 <- c(stopwords("english"), "also", "using", "use", "used")
```

------------------------------------------------------------------------

# Question 4: ngrams

Repeat question 2, but this time tokenize into bi-grams. How does the result change if you look at tri-grams? Note we need to remove stopwords a little differently. You don't need to recreate the wordclouds.

```{r, warning=FALSE}
sw_start <- paste0("^", paste(stopwords2, collapse=" |^"), "$")
sw_end <- paste0("", paste(stopwords2, collapse="$| "), "$")

tokens_bigram <- mt_samples |>
  select(transcription) |>
  unnest_tokens(ngram, transcription, token = "ngrams", n = 2) |>
  filter(!(grepl(sw_start, ngram, ignore.case=TRUE)))|>
  filter(!(grepl(sw_end, ngram, ignore.case=TRUE)))|>
  filter(!(grepl("[[:digit:]]+", ngram)))|>
  group_by(ngram) |>
  summarise(word_frequency=n()) |>
  arrange(across(word_frequency, desc)) |>
  head(20)

tokens_bigram |>
  ggplot(aes(reorder(ngram, word_frequency), word_frequency)) +
  geom_bar(stat = "identity") +
  labs(x = "Bi-grams", y = "Frequency", title = "Top 20 Most Frequent Words of Bi-gram Frequency") +
  coord_flip()

wordcloud(tokens_bigram$ngram, tokens_bigram$word_frequency)

# tri-gram bar plot, word cloud
# change stop word
# For tri-grams, we need to add sw_middle
sw_start <- paste0("^", paste(stopwords2, collapse=" |^"), "$")
sw_middle <- paste0("\\s", paste(stopwords2, collapse="\\s|\\s"), "\\s")
sw_end <- paste0("", paste(stopwords2, collapse="$| "), "$")

tokens_trigram <- mt_samples |>
  select(transcription) |>
  unnest_tokens(ngram, transcription, token = "ngrams", n = 3) |>
  filter(!grepl(sw_start, ngram, ignore.case=TRUE))|>
  filter(!grepl(sw_middle, ngram, ignore.case=TRUE))|>
  filter(!grepl(sw_end, ngram, ignore.case=TRUE))|>
  filter(!grepl("[[:digit:]]+", ngram))|>
  group_by(ngram) |>
  summarise(word_frequency = n()) |>
  arrange(across(word_frequency, desc)) |>
  head(20)

tokens_trigram |>
  ggplot(aes(reorder(ngram, word_frequency), word_frequency)) +
  geom_bar(stat = "identity") +
  labs(y="Frequency",
       x="Tri-grams",
       title="Top 20 Most Frequent Words of Tri-gram Frequency") +
  coord_flip()

wordcloud(tokens_trigram$ngram, tokens_trigram$word_frequency)
```

Tri-gram contains more details than bi-gram.

------------------------------------------------------------------------

# Question 5: Examining words

Using the results from the bigram, pick a word and count the words that appear before and after it, and create a plot of the top 20.

```{r}
library(stringr)
# e.g. patient, blood, preoperative...
# "[word] blood" | "blood [word]"
# "\\sblood$ | ^blood\\s"

tokens_bigram <- mt_samples |>
  select(transcription) |>
  unnest_tokens(ngram, transcription, token = "ngrams", n = 2) |>
  filter(!grepl(sw_start, ngram, ignore.case=TRUE))|>
  filter(!grepl(sw_end, ngram, ignore.case=TRUE))|>
  filter(!grepl("[[:digit:]]+", ngram))|>
  group_by(ngram) |>
  summarise(ngram_frequency = n()) |>
  arrange(across(ngram_frequency, desc))

tokens_blood <- tokens_bigram |>
  filter(str_detect(ngram, "\\sblood$|^blood\\s")) |>
    mutate(word = str_remove(ngram, "blood"),
         word = str_remove_all(word, " ")) |>
  group_by(word) |>
  summarise(word_frequency = n()) |>
  arrange(across(word_frequency, desc)) |>
  head(20)

tokens_blood |>
  ggplot(aes(reorder(word, word_frequency), word_frequency)) +
  geom_bar(stat = "identity") +
  labs(x = "Word", y = "Frequency", title = "Top 20 Most Frequent Words") +
  coord_flip()
```

------------------------------------------------------------------------

# Question 6: Words by Specialties

Which words are most used in each of the specialties. you can use `group_by()` and `top_n()` from `dplyr` to have the calculations be done within each specialty. Remember to remove stopwords. How about the most 5 used words?

```{r}
# the most used word
mt_samples |>
   unnest_tokens(word, transcription) |>
   filter(!(word %in% stopwords("english"))) |>
   filter(!str_detect(word, "[[:digit:]]+")) |>
   group_by(medical_specialty) |>
   count(word, sort=TRUE) |>
   top_n(1, n)

# the most 5 used word
mt_samples |>
   unnest_tokens(word, transcription) |>
   filter(!(word %in% stopwords("english"))) |>
   filter(!str_detect(word, "[[:digit:]]+")) |>
   group_by(medical_specialty) |>
   count(word, sort=TRUE) |>
   top_n(5, n)
```

# Question 7: Topic Models

See if there are any themes in the data by using a topic model (LDA).

-   you first need to create a document term matrix
-   then you can try the LDA function in `topicmodels`. Try different k values.
-   create a facet plot of the results from the LDA (see code from lecture)

```{r}
transcripts_dtm <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  filter(!(word %in% stopwords("english"))) |>
  filter(!(grepl("[[:digit:]]+", word))) |>
  DocumentTermMatrix()


transcripts_dtm <- as.matrix(transcripts_dtm)   

transcripts_lda <- LDA(transcripts_dtm, k=5, control = list(seed = 1234))
transcripts_lda
```

```{r}
transcripts_tops_terms <-
  tidy(transcripts_lda, matrix='beta') |>
  group_by(topic) |>
  slice_max(beta, n=10) |>
  ungroup() |>
  arrange(topic, -beta)

transcripts_tops_terms |>
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

# Deliverables

1.  Questions 1-7 answered, raw .Rmd file and pdf or html output uploaded to Quercus
